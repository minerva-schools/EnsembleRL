{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTf4ilCVUX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i_ULZd1jUbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import scipy.stats as sts\n",
        "import seaborn as sns\n",
        "import math\n",
        "import rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fg2RwckXU2p",
        "colab_type": "text"
      },
      "source": [
        "# Agents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T44ua889Voqo",
        "colab_type": "text"
      },
      "source": [
        "Classic Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExWPDb-nXWsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os \n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Ensemble RL\")\n",
        "\n",
        "from sarsa_agent import SARSAAgent\n",
        "from reinforce import REINFORCE\n",
        "from q_learning import CartPoleQAgent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGWPawZfetu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_q_learning():\n",
        "    agent = CartPoleQAgent()\n",
        "    steps = agent.train()\n",
        "\n",
        "    return agent \n",
        "\n",
        "q_agent = load_q_learning()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHthJ6J9VfQY",
        "colab_type": "text"
      },
      "source": [
        "Deep REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGeNnRTV5og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Config ##\n",
        "import tensorflow as tf\n",
        "ENV=\"CartPole-v1\"\n",
        "RANDOM_SEED=1\n",
        "N_EPISODES=500\n",
        "\n",
        "# random seed (reproduciblity)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "# tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# set the env\n",
        "env=gym.make(ENV) # env to import\n",
        "env.seed(RANDOM_SEED)\n",
        "env.reset() # reset to env \n",
        "reinforce_agent=REINFORCE(env)\n",
        "reinforce_agent.load_model(\"./model.h5\") #Available to download here: https://drive.google.com/open?id=16MYB_Hy_gdVlGn-ianKduIYOuU1BjK9L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_nCum7NWVMn",
        "colab_type": "text"
      },
      "source": [
        "Deep Sarsa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPU7_n8BtZZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "seed_val = 456\n",
        "env.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "#Getting the state and action space\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "\n",
        "#Defining a Neural Network function for our Cartpole agent \n",
        "def agent(states, actions):\n",
        "    \"\"\"Creating a simple Deep Neural Network.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape = (1, states)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "#Getting our neural network\n",
        "model = agent(states, actions)\n",
        "#Defining SARSA Keras-RL agent: inputing the policy and the model\n",
        "sarsa = SARSAAgent(model=model, nb_actions=actions, policy=EpsGreedyQPolicy())\n",
        "#Compiling SARSA with mean squared error loss\n",
        "sarsa.compile('adam', metrics=[\"mse\"])\n",
        "\n",
        "#Training the agent for 50000 steps\n",
        "sarsa.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZiL4da9DiMJ",
        "colab_type": "text"
      },
      "source": [
        "# Ensembling Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8TM5FamBzEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def majority_vote(p1, p2, p3):\n",
        "    '''\n",
        "    Takes three different probability vectors in and outputs a randomly sampled \n",
        "    action from n_action according to majority voting scheme\n",
        "    '''\n",
        "    a = range(n_action)\n",
        "    a1 = np.random.choice(a=a, p=p1)\n",
        "    a2 = np.random.choice(a=a, p=p2)\n",
        "    a3 = np.random.choice(a=a, p=p3)\n",
        "    l = [a1, a2, a3]\n",
        "    return max(set(l), key=l.count)\n",
        "\n",
        "def average_prob(p1, p2, p3):\n",
        "    '''\n",
        "    Takes three different probability vectors in and outputs a randomly sampled \n",
        "    action from n_action with probability equals the average probability of the\n",
        "    input vectors\n",
        "    '''\n",
        "    a = range(n_action)\n",
        "    p = (p1 + p2 + p3)/3\n",
        "    p = p/np.sum(p)\n",
        "    a = np.random.choice(a=a, p=p)\n",
        "    return a\n",
        "\n",
        "def boltzmann_prob(p1, p2, p3, T=0.5):\n",
        "    '''\n",
        "    Takes three different probability vectors in and outputs a randomly sampled \n",
        "    action from n_action with probability equals the average probability of the \n",
        "    normalized exponentiated input vectors, with a temperature T controlling\n",
        "    the degree of spread for the out vector\n",
        "    '''\n",
        "    a = range(n_action)\n",
        "    boltz_ps = [np.exp(prob/T)/sum(np.exp(prob/T)) for prob in [p1, p2, p3]]\n",
        "    p = (boltz_ps[0] + boltz_ps[1] + boltz_ps[2])/3\n",
        "    p = p/np.sum(p)\n",
        "    a = np.random.choice(a=a, p=p)\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy2hMOrvaBcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_action = 2\n",
        "def ensembler_play(learners, env, episodes, vote=\"majority_vote\"):\n",
        "  '''\n",
        "  Takes in the agents, the environment and number of episodes to perform\n",
        "  ensemble learning for some episodes of play from the environment\n",
        "  '''\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      _, p = learners[0].get_action(state)\n",
        "      ps.append(p)\n",
        "      p = learners[1].get_action(state, 500)\n",
        "      ps.append((p + np.max(p) + 1)/np.sum(p + np.max(p) + 1))\n",
        "      q_values = learners[2].compute_q_values(state.reshape(1, 4))\n",
        "      q_values = q_values.reshape((1, 2))\n",
        "      probs=q_values[0]\n",
        "      probs/=np.sum(probs)\n",
        "      ps.append(probs)\n",
        "      # print(ps)\n",
        "      if vote == \"majority_vote\":\n",
        "          action = majority_vote(ps[0], ps[1], ps[2])\n",
        "      elif vote == \"average_prob\":\n",
        "          action = average_prob(ps[0], ps[1], ps[2])\n",
        "      elif vote == \"boltzmann_prob\":\n",
        "          action = boltzmann_prob(ps[0], ps[1], ps[2])\n",
        "      else: raise Exception(\"Not implemented voting scheme\")\n",
        "    \n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUyEa8-_v7Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agents = [reinforce_agent, q_agent, sarsa]\n",
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(ensembler_play(agents, env, 100))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXqn_eu1wVPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(ensembler_play(agents, env, 100, 'average_prob'))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOXIAqybwidz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(ensembler_play(agents, env, 100, 'boltzmann_prob'))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOeWEnbC5rTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learner_play1(learner, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      _, p = learner.get_action(state)\n",
        "      action = np.argmax(p)\n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)\n",
        "\n",
        "\n",
        "def learner_play2(learner, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      p = learner.get_action(state, 500)\n",
        "      (p + np.max(p) + 1)/np.sum(p + np.max(p) + 1)\n",
        "      action = np.argmax(p)\n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)\n",
        "\n",
        "def learner_play3(learner, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      q_values = learner.compute_q_values(state.reshape(1, 4))\n",
        "      q_values = q_values.reshape((1, 2))\n",
        "      probs=q_values[0]\n",
        "      probs/=np.sum(probs)\n",
        "      action = np.argmax(probs)\n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F5U72Vc6FJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(learner_play1(agents[0], env, 40))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpn6Oxca6SPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  if _ % 100 == 0:\n",
        "      print(_)\n",
        "  r.append(learner_play2(q_agent, env, 100))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uU5dIOUdU72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(learner_play3(sarsa, env, 100))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}