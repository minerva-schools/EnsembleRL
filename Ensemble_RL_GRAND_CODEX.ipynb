{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble RL: GRAND CODEX.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTf4ilCVUX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i_ULZd1jUbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import scipy.stats as sts\n",
        "import seaborn as sns\n",
        "import math\n",
        "import rl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fg2RwckXU2p",
        "colab_type": "text"
      },
      "source": [
        "# Agents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T44ua889Voqo",
        "colab_type": "text"
      },
      "source": [
        "Classic Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExWPDb-nXWsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os \n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Ensemble RL\")\n",
        "\n",
        "from sarsa_agent import SARSAAgent\n",
        "from reinforce import REINFORCE\n",
        "from q_learning import CartPoleQAgent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGWPawZfetu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Base code taken from: \n",
        "https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py\n",
        "\"\"\"\n",
        "\n",
        "class CartPoleQAgent():\n",
        "    def __init__(self, buckets=(3, 3, 6, 6), \n",
        "                 num_episodes=500, min_lr=0.1, \n",
        "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
        "        self.buckets = buckets\n",
        "        self.num_episodes = num_episodes\n",
        "        self.min_lr = min_lr\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.discount = discount\n",
        "        self.decay = decay\n",
        "\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        \n",
        "        # This is the action-value function being initialized to 0's\n",
        "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
        "\n",
        "        # [position, velocity, angle, angular velocity]\n",
        "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
        "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
        "        \n",
        "        #\n",
        "        self.steps = np.zeros(self.num_episodes)\n",
        "        \n",
        "        \n",
        "\n",
        "    def discretize_state(self, obs):\n",
        "        \"\"\"\n",
        "        Takes an observation of the environment and aliases it.\n",
        "        By doing this, very similar observations can be treated\n",
        "        as the same and it reduces the state space so that the \n",
        "        Q-table can be smaller and more easily filled.\n",
        "        \n",
        "        Input:\n",
        "        obs (tuple): Tuple containing 4 floats describing the current\n",
        "                     state of the environment.\n",
        "        \n",
        "        Output:\n",
        "        discretized (tuple): Tuple containing 4 non-negative integers smaller \n",
        "                             than n where n is the number in the same position\n",
        "                             in the buckets list.\n",
        "        \"\"\"\n",
        "        discretized = list()\n",
        "        for i in range(len(obs)):\n",
        "            scaling = ((obs[i] + abs(self.lower_bounds[i])) \n",
        "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
        "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
        "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
        "            discretized.append(new_obs)\n",
        "        return tuple(discretized)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Implementation of e-greedy algorithm. Returns an action (0 or 1).\n",
        "        \n",
        "        \n",
        "        Input:\n",
        "        state (tuple): Tuple containing 4 non-negative integers within\n",
        "                       the range of the buckets.\n",
        "        \n",
        "        Output:\n",
        "        (int) Returns either 0 or 1\n",
        "        \"\"\"\n",
        "        if (np.random.random() < self.epsilon):\n",
        "            return self.env.action_space.sample() \n",
        "        else:\n",
        "            return np.argmax(self.Q_table[state])\n",
        "        \n",
        "    def get_action(self, state, e):\n",
        "        \"\"\"\n",
        "        Another policy based on the Q-table. Slight variation from \n",
        "        e-greedy. It assumes the state fed hasn't been discretized and \n",
        "        returns a vector with probabilities for each action.\n",
        "        \n",
        "        Input: \n",
        "        state (tuple): Contains the 4 floats used to describe\n",
        "                       the current state of the environment.\n",
        "        e (int): Denotes the episode at which the agent is supposed\n",
        "                 to be, helping balance exploration and exploitation.\n",
        "                 \n",
        "        Output:\n",
        "        action_vector (numpy array): Vector containing the probability\n",
        "                                     of each action being chosen at the\n",
        "                                     current state.\n",
        "        \"\"\"\n",
        "        obs = self.discretize_state(state)\n",
        "        action_vector = self.Q_table[obs]\n",
        "        epsilon = self.get_epsilon(e)\n",
        "        action_vector = self.normalize(action_vector, epsilon)\n",
        "        return action_vector\n",
        "\n",
        "    def normalize(self, action_vector, epsilon):\n",
        "        \"\"\"\n",
        "        Returns a vector with components adding to 1. Ensures \n",
        "        \n",
        "        Input:\n",
        "        action_vector (numpy array): Contains expected values for each\n",
        "                                     action at current state from Q-table.\n",
        "        epsilon (float): Chances that the e-greedy algorithm would \n",
        "                         choose an action at random. With this pol\n",
        "        \n",
        "        Output:\n",
        "        new_vector (numpy array): Vector containing the probability\n",
        "                                  of each action being chosen at the\n",
        "                                  current state.\n",
        "        \"\"\"\n",
        "        \n",
        "        total = sum(action_vector)\n",
        "        new_vector = (1-epsilon)*action_vector/(total)\n",
        "        new_vector += epsilon/2.0\n",
        "        return new_vector\n",
        "\n",
        "    def update_q(self, state, action, reward, new_state):\n",
        "        \"\"\"\n",
        "        Updates Q-table using the rule as described by Sutton and Barto in\n",
        "        Reinforcement Learning.\n",
        "        \"\"\"\n",
        "        self.Q_table[state][action] += self.learning_rate * (reward + self.discount * np.max(self.Q_table[new_state]) - self.Q_table[state][action])\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
        "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "    def get_learning_rate(self, t):\n",
        "        \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
        "        # Learning rate also declines as we add more episodes\n",
        "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains agent making it go through the environment and choose actions\n",
        "        through an e-greedy policy and updating values for its Q-table. The \n",
        "        agent is trained by default for 500 episodes with a declining \n",
        "        learning rate and epsilon values that with the default values,\n",
        "        reach the minimum after 198 episodes.\n",
        "        \"\"\"\n",
        "        # Looping for each episode\n",
        "        for e in range(self.num_episodes):\n",
        "            # Initializes the state\n",
        "            current_state = self.discretize_state(self.env.reset())\n",
        "\n",
        "            self.learning_rate = self.get_learning_rate(e)\n",
        "            self.epsilon = self.get_epsilon(e)\n",
        "            done = False\n",
        "            \n",
        "            # Looping for each step\n",
        "            while not done:\n",
        "                self.steps[e] += 1\n",
        "                # Choose A from S\n",
        "                action = self.choose_action(current_state)\n",
        "                # Take action\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                # Update Q(S,A)\n",
        "                self.update_q(current_state, action, reward, new_state)\n",
        "                current_state = new_state\n",
        "                \n",
        "                # We break out of the loop when done is False which is\n",
        "                # a terminal state.\n",
        "\n",
        "        print('Finished training!')\n",
        "    \n",
        "    def plot_learning(self):\n",
        "        \"\"\"\n",
        "        Plots the number of steps at each episode and prints the\n",
        "        amount of times that an episode was successfully completed.\n",
        "        \"\"\"\n",
        "        sns.lineplot(range(len(self.steps)),self.steps)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Steps\")\n",
        "        plt.show()\n",
        "        t = 0\n",
        "        for i in range(self.num_episodes):\n",
        "            if self.steps[i] == 200:\n",
        "                t+=1\n",
        "        print(t, \"episodes were successfully completed.\")\n",
        "        \n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
        "        self.env = gym.wrappers.Monitor(self.env,'cartpole')\n",
        "        t = 0\n",
        "        done = False\n",
        "        current_state = self.discretize_state(self.env.reset())\n",
        "        while not done:\n",
        "                self.env.render()\n",
        "                t = t+1\n",
        "                action = self.choose_action(current_state)\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                current_state = new_state\n",
        "            \n",
        "        return t   \n",
        "\n",
        "\n",
        "def load_q_learning():\n",
        "    agent = CartPoleQAgent()\n",
        "    steps = agent.train()\n",
        "\n",
        "    return agent\n",
        "\n",
        "q_agent = load_q_learning()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHthJ6J9VfQY",
        "colab_type": "text"
      },
      "source": [
        "Deep REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGeNnRTV5og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Config ##\n",
        "import tensorflow as tf\n",
        "ENV=\"CartPole-v1\"\n",
        "RANDOM_SEED=1\n",
        "N_EPISODES=500\n",
        "\n",
        "# random seed (reproduciblity)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "# tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# set the env\n",
        "env=gym.make(ENV) # env to import\n",
        "env.seed(RANDOM_SEED)\n",
        "env.reset() # reset to env \n",
        "\n",
        "class REINFORCE:\n",
        "  def __init__(self, env, path=None):\n",
        "    self.env=env #import env\n",
        "    self.state_shape=env.observation_space.shape # the state space\n",
        "    self.action_shape=env.action_space.n # the action space\n",
        "    self.gamma=0.99 # decay rate of past observations\n",
        "    self.alpha=1e-4 # learning rate in the policy gradient\n",
        "    self.learning_rate=0.01 # learning rate in deep learning\n",
        "    \n",
        "    if not path:\n",
        "      self.model=self._create_model() #build model\n",
        "    else:\n",
        "      self.model=self.load_model(path) #import model\n",
        "\n",
        "    # record observations\n",
        "    self.states=[]\n",
        "    self.gradients=[] \n",
        "    self.rewards=[]\n",
        "    self.probs=[]\n",
        "    self.discounted_rewards=[]\n",
        "    self.total_rewards=[]\n",
        "  \n",
        "  def _create_model(self):\n",
        "    ''' builds the model using keras'''\n",
        "    model=Sequential()\n",
        "\n",
        "    # input shape is of observations\n",
        "    model.add(Dense(24, input_shape=self.state_shape, activation=\"relu\"))\n",
        "    #model.add(Dropout(0.5))\n",
        "    # introduce a relu layer \n",
        "    model.add(Dense(12, activation=\"relu\"))\n",
        "    #model.add(Dropout(0.5))    \n",
        "\n",
        "    # output shape is according to the number of action\n",
        "    # The softmax function outputs a probability distribution over the actions\n",
        "    model.add(Dense(self.action_shape, activation=\"softmax\")) \n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "            optimizer=Adam(lr=self.learning_rate))\n",
        "        \n",
        "    return model\n",
        "\n",
        "  def hot_encode_action(self, action):\n",
        "    '''encoding the actions into a binary list'''\n",
        "\n",
        "    action_encoded=np.zeros(self.action_shape, np.float32)\n",
        "    action_encoded[action]=1\n",
        "\n",
        "    return action_encoded\n",
        "  \n",
        "  def remember(self, state, action, action_prob, reward):\n",
        "    '''stores observations'''\n",
        "    encoded_action=self.hot_encode_action(action)\n",
        "    self.gradients.append(encoded_action-action_prob)\n",
        "    self.states.append(state)\n",
        "    self.rewards.append(reward)\n",
        "    self.probs.append(action_prob)\n",
        "\n",
        "  \n",
        "  def get_action(self, state):\n",
        "    '''samples the next action based on the policy probabilty distribution \n",
        "      of the actions'''\n",
        "\n",
        "    # transform state\n",
        "    state=state.reshape([1, state.shape[0]])\n",
        "    # get action probably\n",
        "    action_probability_distribution=self.model.predict(state).flatten()\n",
        "    # norm action probability distribution\n",
        "    action_probability_distribution/=np.sum(action_probability_distribution)\n",
        "    \n",
        "    # sample action\n",
        "    action=np.random.choice(self.action_shape,1,\n",
        "                            p=action_probability_distribution)[0]\n",
        "\n",
        "    return action, action_probability_distribution\n",
        "\n",
        "\n",
        "  def get_discounted_rewards(self, rewards): \n",
        "    '''Use gamma to calculate the total reward discounting for rewards\n",
        "    Following - \\gamma ^ t * Gt'''\n",
        "    \n",
        "    discounted_rewards=[]\n",
        "    cumulative_total_return=0\n",
        "    # iterate the rewards backwards and and calc the total return \n",
        "    for reward in rewards[::-1]:      \n",
        "      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n",
        "      discounted_rewards.insert(0, cumulative_total_return)\n",
        "\n",
        "    # normalize discounted rewards\n",
        "    mean_rewards=np.mean(discounted_rewards)\n",
        "    std_rewards=np.std(discounted_rewards)\n",
        "    norm_discounted_rewards=(discounted_rewards-\n",
        "                          mean_rewards)/(std_rewards+1e-7) # avoiding zero div\n",
        "    \n",
        "    return norm_discounted_rewards\n",
        "\n",
        "\n",
        "  def update_policy(self):\n",
        "    '''Updates the policy network using the NN model.\n",
        "    This function is used after the MC sampling is done - following\n",
        "    \\delta \\theta = \\alpha * gradient + log pi'''\n",
        "      \n",
        "    # get X\n",
        "    states=np.vstack(self.states)\n",
        "\n",
        "    # get Y\n",
        "    gradients=np.vstack(self.gradients)\n",
        "    rewards=np.vstack(self.rewards)\n",
        "    discounted_rewards=self.get_discounted_rewards(rewards)\n",
        "    gradients*=discounted_rewards\n",
        "    gradients=self.alpha*np.vstack([gradients])+self.probs\n",
        "\n",
        "    history=self.model.train_on_batch(states, gradients)\n",
        "    \n",
        "    self.states, self.probs, self.gradients, self.rewards=[], [], [], []\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "  def train(self, episodes, rollout_n=1, render_n=50):\n",
        "    '''train the model\n",
        "        episodes - number of training iterations \n",
        "        rollout_n- number of episodes between policy update\n",
        "        render_n - number of episodes between env rendering ''' \n",
        "    \n",
        "    env=self.env\n",
        "    total_rewards=np.zeros(episodes)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "      # each episode is a new game env\n",
        "      state=env.reset()\n",
        "      done=False          \n",
        "      episode_reward=0 #record episode reward\n",
        "      \n",
        "      while not done:\n",
        "        # play an action and record the game state & reward per episode\n",
        "        action, prob=self.get_action(state)\n",
        "        next_state, reward, done, _=env.step(action)\n",
        "        self.remember(state, action, prob, reward)\n",
        "        state=next_state\n",
        "        episode_reward+=reward\n",
        "\n",
        "        #if episode%render_n==0: ## render env to visualize.\n",
        "          #env.render()\n",
        "        if done:\n",
        "          # update policy \n",
        "          if episode%rollout_n==0:\n",
        "            history=self.update_policy()\n",
        "\n",
        "      total_rewards[episode]=episode_reward\n",
        "      \n",
        "    self.total_rewards=total_rewards\n",
        "\n",
        "  def save_model(self):\n",
        "    '''saves the moodel // do after training'''\n",
        "    self.model.save('REINFORCE_model.h5')\n",
        "  \n",
        "  def load_model(self, path):\n",
        "    '''loads a trained model from path'''\n",
        "    return load_model(path)\n",
        "\n",
        "reinforce_agent=REINFORCE(env)\n",
        "reinforce_agent.load_model(\"./model.h5\") #Available to download here: https://drive.google.com/open?id=16MYB_Hy_gdVlGn-ianKduIYOuU1BjK9L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_nCum7NWVMn",
        "colab_type": "text"
      },
      "source": [
        "Deep Sarsa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPU7_n8BtZZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "%tensorflow_version 1.14\n",
        "from keras.layers import Dense, Flatten, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from rl.agents import SARSAAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.callbacks import TrainEpisodeLogger\n",
        "\n",
        "import collections\n",
        "\n",
        "from keras.callbacks import History\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda\n",
        "import keras.backend as K\n",
        "\n",
        "from rl.core import Agent\n",
        "from rl.agents.dqn import mean_q\n",
        "from rl.util import huber_loss\n",
        "from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
        "from rl.util import get_object_config\n",
        "\n",
        "\n",
        "class SARSAAgent(Agent):\n",
        "    \"\"\"This class defines the SARSA agent\n",
        "    \"\"\"\n",
        "    def __init__(self, model, nb_actions, policy, test_policy=EpsGreedyQPolicy(), gamma=.9, nb_steps_warmup=10,\n",
        "                 train_interval=1, delta_clip=np.inf, *args, **kwargs):\n",
        "        super(SarsaAgent, self).__init__(*args, **kwargs)\n",
        "\n",
        "        if policy is None:\n",
        "            policy = EpsGreedyQPolicy()\n",
        "        if test_policy is None:\n",
        "            test_policy = GreedyQPolicy()\n",
        "\n",
        "        self.model = model\n",
        "        self.nb_actions = nb_actions\n",
        "        self.policy = policy\n",
        "        self.test_policy = test_policy\n",
        "        self.gamma = gamma\n",
        "        self.nb_steps_warmup = nb_steps_warmup\n",
        "        self.train_interval = train_interval\n",
        "\n",
        "        self.delta_clip = delta_clip\n",
        "        self.compiled = False\n",
        "        self.actions = None\n",
        "        self.observations = None\n",
        "        self.rewards = None\n",
        "        self.q_values=[]\n",
        "\n",
        "    def compute_batch_q_values(self, state_batch):\n",
        "        batch = self.process_state_batch(state_batch)\n",
        "        q_values = self.model.predict_on_batch(batch)\n",
        "        assert q_values.shape == (len(state_batch), self.nb_actions)\n",
        "        return q_values\n",
        "\n",
        "    def compute_q_values(self, state):\n",
        "        q_values = self.compute_batch_q_values([state]).flatten()\n",
        "        assert q_values.shape == (self.nb_actions,)\n",
        "        return q_values\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        batch = np.array(batch)\n",
        "        if self.processor is None:\n",
        "            return batch\n",
        "        return self.processor.process_state_batch(batch)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SarsaAgent, self).get_config()\n",
        "        config['nb_actions'] = self.nb_actions\n",
        "        config['gamma'] = self.gamma\n",
        "        config['nb_steps_warmup'] = self.nb_steps_warmup\n",
        "        config['train_interval'] = self.train_interval\n",
        "        config['delta_clip'] = self.delta_clip\n",
        "        config['model'] = get_object_config(self.model)\n",
        "        config['policy'] = get_object_config(self.policy)\n",
        "        config['test_policy'] = get_object_config(self.test_policy)\n",
        "        return config\n",
        "\n",
        "    def compile(self, optimizer, metrics=[]):\n",
        "        metrics += [mean_q]  \n",
        "\n",
        "        def clipped_masked_error(args):\n",
        "            y_true, y_pred, mask = args\n",
        "            loss = huber_loss(y_true, y_pred, self.delta_clip)\n",
        "            loss *= mask  \n",
        "            return K.sum(loss, axis=-1)\n",
        "\n",
        "        # Create trainable model. The problem is that we need to mask the output since we only\n",
        "        # ever want to update the Q values for a certain action. The way we achieve this is by\n",
        "        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n",
        "        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n",
        "        y_pred = self.model.output\n",
        "        y_true = Input(name='y_true', shape=(self.nb_actions,))\n",
        "        mask = Input(name='mask', shape=(self.nb_actions,))\n",
        "        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_pred, y_true, mask])\n",
        "        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n",
        "        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])\n",
        "        assert len(trainable_model.output_names) == 2\n",
        "        combined_metrics = {trainable_model.output_names[1]: metrics}\n",
        "        losses = [\n",
        "            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n",
        "            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n",
        "        ]\n",
        "        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n",
        "        self.trainable_model = trainable_model\n",
        "\n",
        "        self.compiled = True\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        self.model.load_weights(filepath)\n",
        "\n",
        "    def save_weights(self, filepath, overwrite=False):\n",
        "        self.model.save_weights(filepath, overwrite=overwrite)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.actions = collections.deque(maxlen=2)\n",
        "        self.observations = collections.deque(maxlen=2)\n",
        "        self.rewards = collections.deque(maxlen=2)\n",
        "        if self.compiled:\n",
        "            self.model.reset_states()\n",
        "\n",
        "    def forward(self, observation):\n",
        "        # Select an action.\n",
        "        q_values = self.compute_q_values([observation])\n",
        "        if self.training:\n",
        "            action = self.policy.select_action(q_values=q_values)\n",
        "        else:\n",
        "            action = self.test_policy.select_action(q_values=q_values)\n",
        "\n",
        "        # Book-keeping.\n",
        "        self.observations.append(observation)\n",
        "        self.actions.append(action)\n",
        "        # self.q_values=q_values\n",
        "\n",
        "        return action\n",
        "\n",
        "    def backward(self, reward, terminal):\n",
        "        metrics = [np.nan for _ in self.metrics_names]\n",
        "        if not self.training:\n",
        "            # We're done here. No need to update the experience memory since we only use the working\n",
        "            # memory to obtain the state over the most recent observations.\n",
        "            return metrics\n",
        "\n",
        "        # Train the network on a single stochastic batch.\n",
        "        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n",
        "            # Start by extracting the necessary parameters (we use a vectorized implementation).\n",
        "            self.rewards.append(reward)\n",
        "            if len(self.observations) < 2:\n",
        "                return metrics  # not enough data yet\n",
        "\n",
        "            state0_batch = [self.observations[0]]\n",
        "            reward_batch = [self.rewards[0]]\n",
        "            action_batch = [self.actions[0]]\n",
        "            terminal1_batch = [0.] if terminal else [1.]\n",
        "            state1_batch = [self.observations[1]]\n",
        "            action1_batch = [self.actions[1]]\n",
        "\n",
        "            # Prepare and validate parameters.\n",
        "            state0_batch = self.process_state_batch(state0_batch)\n",
        "            state1_batch = self.process_state_batch(state1_batch)\n",
        "            terminal1_batch = np.array(terminal1_batch)\n",
        "            reward_batch = np.array(reward_batch)\n",
        "            assert reward_batch.shape == (1,)\n",
        "            assert terminal1_batch.shape == reward_batch.shape\n",
        "            assert len(action_batch) == len(reward_batch)\n",
        "\n",
        "            batch = self.process_state_batch(state1_batch)\n",
        "            q_values = self.compute_q_values(batch)\n",
        "            q_values = q_values.reshape((1, self.nb_actions))\n",
        "            probs=q_values[0]\n",
        "            probs/=np.sum(probs)\n",
        "            self.q_values.append(probs)\n",
        "            # self.q_values/=np.sum(q_values)\n",
        "\n",
        "            q_batch = q_values[0, action1_batch]\n",
        "\n",
        "            assert q_batch.shape == (1,)\n",
        "            targets = np.zeros((1, self.nb_actions))\n",
        "            dummy_targets = np.zeros((1,))\n",
        "            masks = np.zeros((1, self.nb_actions))\n",
        "\n",
        "            # Compute r_t + gamma * Q(s_t+1, a_t+1)\n",
        "            discounted_reward_batch = self.gamma * q_batch\n",
        "            # Set discounted reward to zero for all states that were terminal.\n",
        "            discounted_reward_batch *= terminal1_batch\n",
        "            assert discounted_reward_batch.shape == reward_batch.shape\n",
        "            Rs = reward_batch + discounted_reward_batch\n",
        "            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n",
        "                target[action] = R  # update action with estimated accumulated reward\n",
        "                dummy_targets[idx] = R\n",
        "                mask[action] = 1.  # enable loss for this specific action\n",
        "            targets = np.array(targets).astype('float32')\n",
        "            masks = np.array(masks).astype('float32')\n",
        "\n",
        "            # Finally, perform a single update on the entire batch. We use a dummy target since\n",
        "            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n",
        "            # it is still useful to know the actual target to compute metrics properly.\n",
        "            state0_batch = state0_batch.reshape((1,) + state0_batch.shape)\n",
        "            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n",
        "            metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\n",
        "            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n",
        "            metrics += self.policy.metrics\n",
        "            if self.processor is not None:\n",
        "                metrics += self.processor.metrics\n",
        "        return metrics\n",
        "\n",
        "    @property\n",
        "    def layers(self):\n",
        "        return self.model.layers[:]\n",
        "\n",
        "    @property\n",
        "    def metrics_names(self):\n",
        "        # Throw away individual losses and replace output name since this is hidden from the user.\n",
        "        assert len(self.trainable_model.output_names) == 2\n",
        "        dummy_output_name = self.trainable_model.output_names[1]\n",
        "        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n",
        "        model_metrics = [name.replace(dummy_output_name + '_', '') for name in model_metrics]\n",
        "\n",
        "        names = model_metrics + self.policy.metrics_names[:]\n",
        "        if self.processor is not None:\n",
        "            names += self.processor.metrics_names[:]\n",
        "        return names\n",
        "\n",
        "    @property\n",
        "    def policy(self):\n",
        "        return self.__policy\n",
        "\n",
        "    @policy.setter\n",
        "    def policy(self, policy):\n",
        "        self.__policy = policy\n",
        "        self.__policy._set_agent(self)\n",
        "\n",
        "    @property\n",
        "    def test_policy(self):\n",
        "        return self.__test_policy\n",
        "\n",
        "    @test_policy.setter\n",
        "    def test_policy(self, policy):\n",
        "        self.__test_policy = policy\n",
        "        self.__test_policy._set_agent(self)\n",
        "\n",
        "# Aliases\n",
        "SarsaAgent = SARSAAgent\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "seed_val = 456\n",
        "env.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "#Getting the state and action space\n",
        "states = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "\n",
        "#Defining a Neural Network function for our Cartpole agent \n",
        "def agent(states, actions):\n",
        "    \"\"\"Creating a simple Deep Neural Network.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape = (1, states)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n",
        "\n",
        "#Getting our neural network\n",
        "model = agent(states, actions)\n",
        "#Defining SARSA Keras-RL agent: inputing the policy and the model\n",
        "sarsa = SARSAAgent(model=model, nb_actions=actions, policy=EpsGreedyQPolicy())\n",
        "#Compiling SARSA with mean squared error loss\n",
        "sarsa.compile('adam', metrics=[\"mse\"])\n",
        "\n",
        "#Training the agent for 50000 steps\n",
        "sarsa.fit(env, nb_steps=50000, visualize=False, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZiL4da9DiMJ",
        "colab_type": "text"
      },
      "source": [
        "# Ensembling Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8TM5FamBzEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def majority_vote(p1, p2, p3):\n",
        "    '''\n",
        "    Takes three different probability vectors in and outputs a randomly sampled \n",
        "    action from n_action according to majority voting scheme\n",
        "    '''\n",
        "    a = range(n_action)\n",
        "    a1 = np.random.choice(a=a, p=p1)\n",
        "    a2 = np.random.choice(a=a, p=p2)\n",
        "    a3 = np.random.choice(a=a, p=p3)\n",
        "    l = [a1, a2, a3]\n",
        "    return max(set(l), key=l.count)\n",
        "\n",
        "def average_prob(p1, p2, p3):\n",
        "    '''\n",
        "    Takes three different probability vectors in and outputs a randomly sampled \n",
        "    action from n_action with probability equals the average probability of the\n",
        "    input vectors\n",
        "    '''\n",
        "    a = range(n_action)\n",
        "    p = (p1 + p2 + p3)/3\n",
        "    p = p/np.sum(p)\n",
        "    a = np.random.choice(a=a, p=p)\n",
        "    return a\n",
        "\n",
        "def boltzmann_prob(p1, p2, p3, T=0.5):\n",
        "    '''\n",
        "    Takes three different probability vectors in and outputs a randomly sampled \n",
        "    action from n_action with probability equals the average probability of the \n",
        "    normalized exponentiated input vectors, with a temperature T controlling\n",
        "    the degree of spread for the out vector\n",
        "    '''\n",
        "    a = range(n_action)\n",
        "    boltz_ps = [np.exp(prob/T)/sum(np.exp(prob/T)) for prob in [p1, p2, p3]]\n",
        "    p = (boltz_ps[0] + boltz_ps[1] + boltz_ps[2])/3\n",
        "    p = p/np.sum(p)\n",
        "    a = np.random.choice(a=a, p=p)\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy2hMOrvaBcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_action = 2\n",
        "def ensembler_play(learners, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      _, p = learners[0].get_action(state)\n",
        "      ps.append(p)\n",
        "      p = learners[1].get_action(state, 500)\n",
        "      ps.append((p + np.max(p) + 1)/np.sum(p + np.max(p) + 1))\n",
        "      q_values = learners[2].compute_q_values(state.reshape(1, 4))\n",
        "      q_values = q_values.reshape((1, 2))\n",
        "      probs=q_values[0]\n",
        "      probs/=np.sum(probs)\n",
        "      ps.append(probs)\n",
        "      # print(ps)\n",
        "      if vote == \"majority_vote\":\n",
        "          action = majority_vote(ps[0], ps[1], ps[2])\n",
        "      elif vote == \"average_prob\":\n",
        "          action = average_prob(ps[0], ps[1], ps[2])\n",
        "      elif vote == \"boltzmann_prob\":\n",
        "          action = boltzmann_prob(ps[0], ps[1], ps[2])\n",
        "      else: raise Exception(\"Not implemented voting scheme\")\n",
        "    \n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUyEa8-_v7Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agents = [reinforce_agent, q_agent, sarsa]\n",
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(ensembler_play(agents, env, 100))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXqn_eu1wVPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(ensembler_play(agents, env, 100, 'average_prob'))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOXIAqybwidz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(ensembler_play(agents, env, 100, 'boltzmann_prob'))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOeWEnbC5rTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learner_play1(learner, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      _, p = learner.get_action(state)\n",
        "      action = np.argmax(p)\n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)\n",
        "\n",
        "\n",
        "def learner_play2(learner, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      p = learner.get_action(state, 500)\n",
        "      (p + np.max(p) + 1)/np.sum(p + np.max(p) + 1)\n",
        "      action = np.argmax(p)\n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)\n",
        "\n",
        "def learner_play3(learner, env, episodes, vote=\"majority_vote\"):\n",
        "  rewards = []\n",
        "  n_action = env.action_space.n\n",
        "  for episode in range(episodes):\n",
        "    ep_reward = 0\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    ps = []\n",
        "    while not done:\n",
        "      q_values = learner.compute_q_values(state.reshape(1, 4))\n",
        "      q_values = q_values.reshape((1, 2))\n",
        "      probs=q_values[0]\n",
        "      probs/=np.sum(probs)\n",
        "      action = np.argmax(probs)\n",
        "      next_state, reward, done,info=env.step(action)\n",
        "      ep_reward += reward\n",
        "      state=next_state\n",
        "\n",
        "      if done:\n",
        "        rewards.append(ep_reward)\n",
        "        ep_reward = []\n",
        "        env.reset()\n",
        "  \n",
        "  return np.mean(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F5U72Vc6FJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(learner_play1(agents[0], env, 40))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpn6Oxca6SPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  if _ % 100 == 0:\n",
        "      print(_)\n",
        "  r.append(learner_play2(q_agent, env, 100))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uU5dIOUdU72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = []\n",
        "for _ in range(20):\n",
        "  print(_)\n",
        "  r.append(learner_play3(sarsa, env, 100))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.hist(r)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}