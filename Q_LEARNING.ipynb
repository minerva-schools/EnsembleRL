{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-LEARNING.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZocV9S8ahZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G83ZFbFwafMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Base code taken from: \n",
        "https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py\n",
        "\"\"\"\n",
        "class CartPoleQAgent():\n",
        "    def __init__(self, buckets=(3, 3, 6, 6), \n",
        "                 num_episodes=500, min_lr=0.1, \n",
        "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
        "        self.buckets = buckets\n",
        "        self.num_episodes = num_episodes\n",
        "        self.min_lr = min_lr\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.discount = discount\n",
        "        self.decay = decay\n",
        "\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        \n",
        "        # This is the action-value function being initialized to 0's\n",
        "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
        "\n",
        "        # [position, velocity, angle, angular velocity]\n",
        "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
        "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
        "        \n",
        "        #\n",
        "        self.steps = np.zeros(self.num_episodes)\n",
        "        \n",
        "        \n",
        "\n",
        "    def discretize_state(self, obs):\n",
        "        \"\"\"\n",
        "        Takes an observation of the environment and aliases it.\n",
        "        By doing this, very similar observations can be treated\n",
        "        as the same and it reduces the state space so that the \n",
        "        Q-table can be smaller and more easily filled.\n",
        "        \n",
        "        Input:\n",
        "        obs (tuple): Tuple containing 4 floats describing the current\n",
        "                     state of the environment.\n",
        "        \n",
        "        Output:\n",
        "        discretized (tuple): Tuple containing 4 non-negative integers smaller \n",
        "                             than n where n is the number in the same position\n",
        "                             in the buckets list.\n",
        "        \"\"\"\n",
        "        discretized = list()\n",
        "        for i in range(len(obs)):\n",
        "            scaling = ((obs[i] + abs(self.lower_bounds[i])) \n",
        "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
        "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
        "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
        "            discretized.append(new_obs)\n",
        "        return tuple(discretized)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Implementation of e-greedy algorithm. Returns an action (0 or 1).\n",
        "        \n",
        "        \n",
        "        Input:\n",
        "        state (tuple): Tuple containing 4 non-negative integers within\n",
        "                       the range of the buckets.\n",
        "        \n",
        "        Output:\n",
        "        (int) Returns either 0 or 1\n",
        "        \"\"\"\n",
        "        if (np.random.random() < self.epsilon):\n",
        "            return self.env.action_space.sample() \n",
        "        else:\n",
        "            return np.argmax(self.Q_table[state])\n",
        "        \n",
        "    def get_action(self, state, e):\n",
        "        \"\"\"\n",
        "        Another policy based on the Q-table. Slight variation from \n",
        "        e-greedy. It assumes the state fed hasn't been discretized and \n",
        "        returns a vector with probabilities for each action.\n",
        "        \n",
        "        Input: \n",
        "        state (tuple): Contains the 4 floats used to describe\n",
        "                       the current state of the environment.\n",
        "        e (int): Denotes the episode at which the agent is supposed\n",
        "                 to be, helping balance exploration and exploitation.\n",
        "                 \n",
        "        Output:\n",
        "        action_vector (numpy array): Vector containing the probability\n",
        "                                     of each action being chosen at the\n",
        "                                     current state.\n",
        "        \"\"\"\n",
        "        obs = self.discretize_state(state)\n",
        "        action_vector = self.Q_table[obs]\n",
        "        epsilon = self.get_epsilon(e)\n",
        "        action_vector = self.normalize(action_vector, epsilon)\n",
        "        return action_vector\n",
        "\n",
        "    def normalize(self, action_vector, epsilon):\n",
        "        \"\"\"\n",
        "        Returns a vector with components adding to 1. Ensures \n",
        "        \n",
        "        Input:\n",
        "        action_vector (numpy array): Contains expected values for each\n",
        "                                     action at current state from Q-table.\n",
        "        epsilon (float): Chances that the e-greedy algorithm would \n",
        "                         choose an action at random. With this pol\n",
        "        \n",
        "        Output:\n",
        "        new_vector (numpy array): Vector containing the probability\n",
        "                                  of each action being chosen at the\n",
        "                                  current state.\n",
        "        \"\"\"\n",
        "        \n",
        "        total = sum(action_vector)\n",
        "        new_vector = (1-epsilon)*action_vector/(total)\n",
        "        new_vector += epsilon/2.0\n",
        "        return new_vector\n",
        "\n",
        "    def update_q(self, state, action, reward, new_state):\n",
        "        \"\"\"\n",
        "        Updates Q-table using the rule as described by Sutton and Barto in\n",
        "        Reinforcement Learning.\n",
        "        \"\"\"\n",
        "        self.Q_table[state][action] += self.learning_rate * (reward + self.discount * np.max(self.Q_table[new_state]) - self.Q_table[state][action])\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
        "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "    def get_learning_rate(self, t):\n",
        "        \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
        "        # Learning rate also declines as we add more episodes\n",
        "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains agent making it go through the environment and choose actions\n",
        "        through an e-greedy policy and updating values for its Q-table. The \n",
        "        agent is trained by default for 500 episodes with a declining \n",
        "        learning rate and epsilon values that with the default values,\n",
        "        reach the minimum after 198 episodes.\n",
        "        \"\"\"\n",
        "        # Looping for each episode\n",
        "        for e in range(self.num_episodes):\n",
        "            # Initializes the state\n",
        "            current_state = self.discretize_state(self.env.reset())\n",
        "\n",
        "            self.learning_rate = self.get_learning_rate(e)\n",
        "            self.epsilon = self.get_epsilon(e)\n",
        "            done = False\n",
        "            \n",
        "            # Looping for each step\n",
        "            while not done:\n",
        "                self.steps[e] += 1\n",
        "                # Choose A from S\n",
        "                action = self.choose_action(current_state)\n",
        "                # Take action\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                # Update Q(S,A)\n",
        "                self.update_q(current_state, action, reward, new_state)\n",
        "                current_state = new_state\n",
        "                \n",
        "                # We break out of the loop when done is False which is\n",
        "                # a terminal state.\n",
        "\n",
        "        print('Finished training!')\n",
        "    \n",
        "    def plot_learning(self):\n",
        "        \"\"\"\n",
        "        Plots the number of steps at each episode and prints the\n",
        "        amount of times that an episode was successfully completed.\n",
        "        \"\"\"\n",
        "        sns.lineplot(range(len(self.steps)),self.steps)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Steps\")\n",
        "        plt.show()\n",
        "        t = 0\n",
        "        for i in range(self.num_episodes):\n",
        "            if self.steps[i] == 200:\n",
        "                t+=1\n",
        "        print(t, \"episodes were successfully completed.\")\n",
        "        \n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
        "        self.env = gym.wrappers.Monitor(self.env,'cartpole')\n",
        "        t = 0\n",
        "        done = False\n",
        "        current_state = self.discretize_state(self.env.reset())\n",
        "        while not done:\n",
        "                self.env.render()\n",
        "                t = t+1\n",
        "                action = self.choose_action(current_state)\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                current_state = new_state\n",
        "            \n",
        "        return t   \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o14acbKoageO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_q_learning():\n",
        "    agent = CartPoleQAgent()\n",
        "    agent.train()\n",
        "    agent.plot_learning()\n",
        "\n",
        "    return agent"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}